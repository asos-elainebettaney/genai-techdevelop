{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell with your Azure OpenAI, endpoint URL, and deployment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install json\n",
    "!python -m pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Assistant:\n",
      "Yes, many Azure AI services support customer managed keys, providing customers with control over the encryption and decryption of their data. Some examples of Azure AI services that support customer managed keys include Azure Cognitive Services, Azure Machine Learning, and Azure Personalizer.\n",
      "\n",
      "AI Content safey:\n",
      "('hate', {'filtered': False, 'severity': 'safe'})\n",
      "('self_harm', {'filtered': False, 'severity': 'safe'})\n",
      "('sexual', {'filtered': False, 'severity': 'safe'})\n",
      "('violence', {'filtered': False, 'severity': 'safe'})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"]=\"594f098863634d7f94693bc88a650105\" # TODO copy your AZURE OPENAI KEY\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"]=\"https://uksouth.api.cognitive.microsoft.com/\" # TODO copy your AZURE OPENAI ENDPOINT URL\n",
    "os.environ[\"AZURE_DEPLOYMENT_NAME\"]=\"aifashionassistant-gpt-35-1106\" # TODO copy your Azure OPENAI DEPLOYMENT NAME\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),    \n",
    "  api_version=\"2023-12-01-preview\",\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model= os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer managed keys?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Yes, customer managed keys are supported by Azure OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Do other Azure AI services support this too?\"}\n",
    "     ]\n",
    ")\n",
    "\n",
    "print (\"\\nAI Assistant:\\n%s\\n\" % response.choices[0].message.content)\n",
    "\n",
    "print (\"AI Content safey:\")\n",
    "for safety_category, safety_diagnostic in response.choices[0].content_filter_results.items():\n",
    "    print((safety_category, safety_diagnostic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to make requests to Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(client, prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=os.getenv(\"AZURE_DEPLOYMENT_NAME\"),  \n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Principles\n",
    "- **Principle 1: Write clear and specific instructions**\n",
    "- **Principle 2: Give the model time to “think”**\n",
    "\n",
    "### Tactics\n",
    "\n",
    "#### Tactic 1: Use delimiters to clearly indicate distinct parts of the input\n",
    "- Delimiters can be anything like: ```, \"\"\", < >, `<tag> </tag>`, `:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Assistant:\n",
      "Clear and specific instructions for a model will guide it towards the desired output and reduce the chances of irrelevant or incorrect responses, and longer prompts can provide more clarity and context for the model, leading to more detailed and relevant outputs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\ \n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print (\"\\nAI Assistant:\\n%s\\n\" % response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Assistant:\n",
      "```json\n",
      "{\n",
      "  \"books\": [\n",
      "    {\n",
      "      \"book_id\": 1,\n",
      "      \"title\": \"The Midnight Garden\",\n",
      "      \"author\": \"Eleanor Blackwood\",\n",
      "      \"genre\": \"Fantasy\"\n",
      "    },\n",
      "    {\n",
      "      \"book_id\": 2,\n",
      "      \"title\": \"The Secret of Silver Lake\",\n",
      "      \"author\": \"Julian Montgomery\",\n",
      "      \"genre\": \"Mystery\"\n",
      "    },\n",
      "    {\n",
      "      \"book_id\": 3,\n",
      "      \"title\": \"Echoes of Eternity\",\n",
      "      \"author\": \"Serena Nightingale\",\n",
      "      \"genre\": \"Romance\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Generate a list of three made-up book titles along \\ \n",
    "with their authors and genres. \n",
    "Provide them in JSON format with the following keys: \n",
    "book_id, title, author, genre.\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print (\"\\nAI Assistant:\\n%s\\n\" % response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text 1:\n",
      "\n",
      "AI Assistant:\n",
      "Step 1 - Get some water boiling.\n",
      "Step 2 - Grab a cup and put a tea bag in it.\n",
      "Step 3 - Pour the hot water over the tea bag.\n",
      "Step 4 - Let the tea steep for a few minutes.\n",
      "Step 5 - Remove the tea bag.\n",
      "Step 6 - Add sugar or milk to taste.\n",
      "Step 7 - Enjoy your delicious cup of tea.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_1 = f\"\"\"\n",
    "Making a cup of tea is easy! First, you need to get some \\ \n",
    "water boiling. While that's happening, \\ \n",
    "grab a cup and put a tea bag in it. Once the water is \\ \n",
    "hot enough, just pour it over the tea bag. \\ \n",
    "Let it sit for a bit so the tea can steep. After a \\ \n",
    "few minutes, take out the tea bag. If you \\ \n",
    "like, you can add some sugar or milk to taste. \\ \n",
    "And that's it! You've got yourself a delicious \\ \n",
    "cup of tea to enjoy.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \\ \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\ \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print(\"Completion for Text 1:\")\n",
    "print (\"\\nAI Assistant:\\n%s\\n\" % response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text 2:\n",
      "\n",
      "AI Assistant:\n",
      "No steps provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_2 = f\"\"\"\n",
    "The sun is shining brightly today, and the birds are \\\n",
    "singing. It's a beautiful day to go for a \\ \n",
    "walk in the park. The flowers are blooming, and the \\ \n",
    "trees are swaying gently in the breeze. People \\ \n",
    "are out and about, enjoying the lovely weather. \\ \n",
    "Some are having picnics, while others are playing \\ \n",
    "games or simply relaxing on the grass. It's a \\ \n",
    "perfect day to spend time outdoors and appreciate the \\ \n",
    "beauty of nature.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \\ \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\ \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print(\"Completion for Text 2:\")\n",
    "print (\"\\nAI Assistant:\\n%s\\n\" % response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Assistant:\n",
      "<grandparent>: Resilience is like the mighty oak tree that withstands the fiercest storms, bending but never breaking. It is the ability to bounce back from adversity, to find strength in the face of challenges, and to keep moving forward despite setbacks. Just like the oak tree, resilience grows stronger with each trial it endures.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<child>: Teach me about patience.\n",
    "\n",
    "<grandparent>: The river that carves the deepest \\ \n",
    "valley flows from a modest spring; the \\ \n",
    "grandest symphony originates from a single note; \\ \n",
    "the most intricate tapestry begins with a solitary thread.\n",
    "\n",
    "<child>: Teach me about resilience.\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print (\"\\nAI Assistant:\\n%s\\n\" % response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for prompt 1:\n",
      "\n",
      "AI Assistant:\n",
      "1 - Jack and Jill go on a quest to fetch water from a hilltop well, but misfortune strikes as they both tumble down the hill, yet they return home with comforting embraces and their adventurous spirits undimmed.\n",
      "\n",
      "2 - Jack et Jill partent en quête d'eau d'un puits au sommet d'une colline, mais la malchance frappe alors qu'ils dégringolent tous les deux, mais ils rentrent chez eux avec des étreintes réconfortantes et leurs esprits aventureux intacts.\n",
      "\n",
      "3 - Jack, Jill\n",
      "\n",
      "4 - \n",
      "{\n",
      "  \"french_summary\": \"Jack et Jill partent en quête d'eau d'un puits au sommet d'une colline, mais la malchance frappe alors qu'ils dégringolent tous les deux, mais ils rentrent chez eux avec des étreintes réconfortantes et leurs esprits aventureux intacts.\",\n",
      "  \"num_names\": 2\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "In a charming village, siblings Jack and Jill set out on \\ \n",
    "a quest to fetch water from a hilltop \\ \n",
    "well. As they climbed, singing joyfully, misfortune \\ \n",
    "struck—Jack tripped on a stone and tumbled \\ \n",
    "down the hill, with Jill following suit. \\ \n",
    "Though slightly battered, the pair returned home to \\ \n",
    "comforting embraces. Despite the mishap, \\ \n",
    "their adventurous spirits remained undimmed, and they \\ \n",
    "continued exploring with delight.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Summarize the following text delimited by triple \\\n",
    "backticks with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the following \\\n",
    "keys: french_summary, num_names.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt_1)\n",
    "print(\"Completion for prompt 1:\")\n",
    "print (\"\\nAI Assistant:\\n%s\\n\" % response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completion for prompt 2:\n",
      "\n",
      "AI Assistant:\n",
      "Summary: Jack and Jill, siblings, go on a quest to fetch water from a hilltop well, but misfortune strikes as Jack trips on a stone and tumbles down the hill, with Jill following suit, yet they return home slightly battered but with undimmed adventurous spirits.\n",
      "Translation: Jack et Jill, frère et sœur, partent en quête d'eau d'un puits au sommet d'une colline, mais le malheur frappe lorsque Jack trébuche sur une pierre et dégringole la colline, suivi par Jill, mais ils rentrent à la maison légèrement meurtris mais avec des esprits aventureux indomptés.\n",
      "Names: Jack, Jill\n",
      "Output JSON: {\"french_summary\": \"Jack et Jill, frère et sœur, partent en quête d'eau d'un puits au sommet d'une colline, mais le malheur frappe lorsque Jack trébuche sur une pierre et dégringole la colline, suivi par Jill, mais ils rentrent à la maison légèrement meurtris mais avec des esprits aventureux indomptés.\", \"num_names\": 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = f\"\"\"\n",
    "Your task is to perform the following actions: \n",
    "1 - Summarize the following text delimited by \n",
    "  <> with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the \n",
    "  following keys: french_summary, num_names.\n",
    "\n",
    "Use the following format:\n",
    "Text: <text to summarize>\n",
    "Summary: <summary>\n",
    "Translation: <summary translation>\n",
    "Names: <list of names in summary>\n",
    "Output JSON: <json with summary and num_names>\n",
    "\n",
    "Text: <{text}>\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt_2)\n",
    "print(\"\\nCompletion for prompt 2:\")\n",
    "print (\"\\nAI Assistant:\\n%s\\n\" % response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Assistant:\n",
      "The student's solution is correct. The total cost for the first year of operations as a function of the number of square feet is indeed 450x + 100,000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need \\\n",
    " help working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\ \n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \n",
    "as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print (\"\\nAI Assistant:\\n%s\\n\" % response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Assistant:\n",
      "The total cost for the first year of operations as a function of the number of square feet is:\n",
      "Total cost = Land cost + Solar panel cost + Maintenance cost\n",
      "Total cost = $100x + $250x + $100,000 + $10x\n",
      "Total cost = $360x + $100,000\n",
      "\n",
      "Is the student's solution the same as actual solution just calculated:\n",
      "```\n",
      "Yes\n",
      "```\n",
      "Student grade:\n",
      "```\n",
      "Correct\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to determine if the student's solution \\\n",
    "is correct or not.\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem including the final total. \n",
    "- Then compare your solution to the student's solution \\ \n",
    "and evaluate if the student's solution is correct or not. \n",
    "Don't decide if the student's solution is correct until \n",
    "you have done the problem yourself.\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the student's solution the same as actual solution \\\n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Student grade:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "I'm building a solar power installation and I need help \\\n",
    "working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\\n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \\\n",
    "as a function of the number of square feet.\n",
    "``` \n",
    "Student's solution:\n",
    "```\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "```\n",
    "Actual solution:\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print (\"\\nAI Assistant:\\n%s\\n\" % response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Assistant:\n",
      "The AeroGlide UltraSlim Smart Toothbrush by Boie is a high-tech toothbrush designed to provide a superior cleaning experience. It features an ultra-slim design that allows for easy maneuverability and access to hard-to-reach areas in the mouth. The toothbrush also comes with smart technology that tracks brushing habits and provides real-time feedback to help users improve their oral hygiene routine.\n",
      "\n",
      "The brush head is made from soft, durable bristles that are gentle on the gums and teeth, while still effectively removing plaque and debris. The bristles are also infused with silver to help prevent bacterial growth and keep the brush clean between uses.\n",
      "\n",
      "The AeroGlide UltraSlim Smart Toothbrush is rechargeable and comes with a long-lasting battery, making it convenient for travel and everyday use. It also connects to a smartphone app, allowing users to track their brushing habits and receive personalized recommendations for improving their oral health.\n",
      "\n",
      "Overall, the AeroGlide UltraSlim Smart Toothbrush by Boie offers a combination of advanced technology and thoughtful design to provide a superior brushing experience for users.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print (\"\\nAI Assistant:\\n%s\\n\" % response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try experimenting on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamp_review = \"\"\"\n",
    "Needed a nice lamp for my bedroom, and this one had \\\n",
    "additional storage and not too high of a price point. \\\n",
    "Got it fast.  The string to our lamp broke during the \\\n",
    "transit and the company happily sent over a new one. \\\n",
    "Came within a few days as well. It was easy to put \\\n",
    "together.  I had a missing part, so I contacted their \\\n",
    "support and they very quickly got me the missing piece! \\\n",
    "Lumina seems to me to be a great company that cares \\\n",
    "about their customers and products!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment (positive/negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following product review, \n",
    "which is delimited with triple backticks?\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify types of emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify a list of emotions that the writer of the \\\n",
    "following review is expressing. Include no more than \\\n",
    "five items in the list. Format your answer as a list of \\\n",
    "lower-case words separated by commas.\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Is the writer of the following review expressing anger?\\\n",
    "The review is delimited with triple backticks. \\\n",
    "Give your answer as either yes or no.\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract product and company name from customer reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify the following items from the review text: \n",
    "- Item purchased by reviewer\n",
    "- Company that made the item\n",
    "\n",
    "The review is delimited with triple backticks. \\\n",
    "Format your response as a JSON object with \\\n",
    "\"Item\" and \"Brand\" as the keys. \n",
    "If the information isn't present, use \"unknown\" \\\n",
    "as the value.\n",
    "Make your response as short as possible.\n",
    "  \n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing multiple tasks at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify the following items from the review text: \n",
    "- Sentiment (positive or negative)\n",
    "- Is the reviewer expressing anger? (true or false)\n",
    "- Item purchased by reviewer\n",
    "- Company that made the item\n",
    "\n",
    "The review is delimited with triple backticks. \\\n",
    "Format your response as a JSON object with \\\n",
    "\"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys.\n",
    "If the information isn't present, use \"unknown\" \\\n",
    "as the value.\n",
    "Make your response as short as possible.\n",
    "Format the Anger value as a boolean.\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "In a recent survey conducted by the government, \n",
    "public sector employees were asked to rate their level \n",
    "of satisfaction with the department they work at. \n",
    "The results revealed that NASA was the most popular \n",
    "department with a satisfaction rating of 95%.\n",
    "\n",
    "One NASA employee, John Smith, commented on the findings, \n",
    "stating, \"I'm not surprised that NASA came out on top. \n",
    "It's a great place to work with amazing people and \n",
    "incredible opportunities. I'm proud to be a part of \n",
    "such an innovative organization.\"\n",
    "\n",
    "The results were also welcomed by NASA's management team, \n",
    "with Director Tom Johnson stating, \"We are thrilled to \n",
    "hear that our employees are satisfied with their work at NASA. \n",
    "We have a talented and dedicated team who work tirelessly \n",
    "to achieve our goals, and it's fantastic to see that their \n",
    "hard work is paying off.\"\n",
    "\n",
    "The survey also revealed that the \n",
    "Social Security Administration had the lowest satisfaction \n",
    "rating, with only 45% of employees indicating they were \n",
    "satisfied with their job. The government has pledged to \n",
    "address the concerns raised by employees in the survey and \n",
    "work towards improving job satisfaction across all departments.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine five topics that are being discussed in the \\\n",
    "following text, which is delimited by triple backticks.\n",
    "\n",
    "Make each item one or two words long. \n",
    "\n",
    "Format your response as a list of items separated by commas.\n",
    "\n",
    "Text sample: '''{story}'''\n",
    "\"\"\"\n",
    "response = get_completion(client, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try experimenting on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your dev environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting promptflow\n",
      "  Using cached promptflow-1.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting promptflow-tools\n",
      "  Using cached promptflow_tools-1.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: psutil in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from promptflow) (5.9.0)\n",
      "Collecting httpx>=0.25.1 (from promptflow)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting openai (from promptflow)\n",
      "  Using cached openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting flask<4.0.0,>=2.2.3 (from promptflow)\n",
      "  Using cached flask-3.0.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting sqlalchemy<3.0.0,>=1.4.48 (from promptflow)\n",
      "  Downloading SQLAlchemy-2.0.27-cp311-cp311-macosx_10_9_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting pandas<3.0.0,>=1.5.3 (from promptflow)\n",
      "  Downloading pandas-2.2.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (19 kB)\n",
      "Collecting python-dotenv<2.0.0,>=1.0.0 (from promptflow)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting keyring<25.0.0,>=24.2.0 (from promptflow)\n",
      "  Using cached keyring-24.3.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting pydash<8.0.0,>=6.0.0 (from promptflow)\n",
      "  Using cached pydash-7.0.7-py3-none-any.whl.metadata (45 kB)\n",
      "Requirement already satisfied: cryptography<42.0.0,>=41.0.3 in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from promptflow) (41.0.7)\n",
      "Collecting colorama<0.5.0,>=0.4.6 (from promptflow)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting tabulate<1.0.0,>=0.9.0 (from promptflow)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting filelock<4.0.0,>=3.4.0 (from promptflow)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.5 (from promptflow)\n",
      "  Using cached marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting gitpython<4.0.0,>=3.1.24 (from promptflow)\n",
      "  Downloading GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tiktoken>=0.4.0 (from promptflow)\n",
      "  Downloading tiktoken-0.6.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting strictyaml<2.0.0,>=1.5.0 (from promptflow)\n",
      "  Downloading strictyaml-1.7.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting waitress<3.0.0,>=2.1.2 (from promptflow)\n",
      "  Using cached waitress-2.1.2-py3-none-any.whl (57 kB)\n",
      "Collecting opencensus-ext-azure<2.0.0 (from promptflow)\n",
      "  Using cached opencensus_ext_azure-1.1.13-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: ruamel.yaml<1.0.0,>=0.17.10 in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from promptflow) (0.17.21)\n",
      "Collecting pyarrow<15.0.0,>=14.0.1 (from promptflow)\n",
      "  Downloading pyarrow-14.0.2-cp311-cp311-macosx_10_14_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pillow<11.0.0,>=10.1.0 (from promptflow)\n",
      "  Downloading pillow-10.2.0-cp311-cp311-macosx_10_10_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting filetype>=1.2.0 (from promptflow)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting jsonschema<5.0.0,>=4.0.0 (from promptflow)\n",
      "  Downloading jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting docutils (from promptflow)\n",
      "  Using cached docutils-0.20.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0 (from promptflow)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.22.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting flask-restx<2.0.0,>=1.2.0 (from promptflow)\n",
      "  Using cached flask_restx-1.3.0-py2.py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting google-search-results==2.4.1 (from promptflow-tools)\n",
      "  Using cached google_search_results-2.4.1.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from google-search-results==2.4.1->promptflow-tools) (2.31.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from cryptography<42.0.0,>=41.0.3->promptflow) (1.16.0)\n",
      "Collecting Werkzeug>=3.0.0 (from flask<4.0.0,>=2.2.3->promptflow)\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting Jinja2>=3.1.2 (from flask<4.0.0,>=2.2.3->promptflow)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting itsdangerous>=2.1.2 (from flask<4.0.0,>=2.2.3->promptflow)\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting click>=8.1.3 (from flask<4.0.0,>=2.2.3->promptflow)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting blinker>=1.6.2 (from flask<4.0.0,>=2.2.3->promptflow)\n",
      "  Using cached blinker-1.7.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting aniso8601>=0.82 (from flask-restx<2.0.0,>=1.2.0->promptflow)\n",
      "  Using cached aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "Collecting pytz (from flask-restx<2.0.0,>=1.2.0->promptflow)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting importlib-resources (from flask-restx<2.0.0,>=1.2.0->promptflow)\n",
      "  Using cached importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4.0.0,>=3.1.24->promptflow)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyio (from httpx>=0.25.1->promptflow)\n",
      "  Downloading anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from httpx>=0.25.1->promptflow) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.1->promptflow)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from httpx>=0.25.1->promptflow) (3.4)\n",
      "Collecting sniffio (from httpx>=0.25.1->promptflow)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.1->promptflow)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema<5.0.0,>=4.0.0->promptflow)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.0.0->promptflow)\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.0.0->promptflow)\n",
      "  Downloading referencing-0.33.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.0.0->promptflow)\n",
      "  Downloading rpds_py-0.18.0-cp311-cp311-macosx_10_12_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting jaraco.classes (from keyring<25.0.0,>=24.2.0->promptflow)\n",
      "  Using cached jaraco.classes-3.3.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting importlib-metadata>=4.11.4 (from keyring<25.0.0,>=24.2.0->promptflow)\n",
      "  Using cached importlib_metadata-7.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.5->promptflow) (23.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from openai->promptflow) (1.8.0)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai->promptflow)\n",
      "  Downloading pydantic-2.6.1-py3-none-any.whl.metadata (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>4 in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from openai->promptflow) (4.65.0)\n",
      "Collecting typing-extensions<5,>=4.7 (from openai->promptflow)\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting azure-core<2.0.0,>=1.12.0 (from opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Using cached azure_core-1.30.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting azure-identity<2.0.0,>=1.5.0 (from opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Using cached azure_identity-1.15.0-py3-none-any.whl.metadata (75 kB)\n",
      "Collecting opencensus<1.0.0,>=0.11.4 (from opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Using cached opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow)\n",
      "  Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-api~=1.15 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow)\n",
      "  Using cached opentelemetry_api-1.22.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.22.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.22.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow)\n",
      "  Using cached opentelemetry_proto-1.22.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk~=1.22.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow)\n",
      "  Using cached opentelemetry_sdk-1.22.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf<5.0,>=3.19 (from opentelemetry-proto==1.22.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting numpy<2,>=1.23.2 (from pandas<3.0.0,>=1.5.3->promptflow)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from pandas<3.0.0,>=1.5.3->promptflow) (2.8.2)\n",
      "Collecting tzdata>=2022.7 (from pandas<3.0.0,>=1.5.3->promptflow)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy<3.0.0,>=1.4.48->promptflow)\n",
      "  Downloading greenlet-3.0.3.tar.gz (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken>=0.4.0->promptflow)\n",
      "  Downloading regex-2023.12.25-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.11.0 in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.12.0->opencensus-ext-azure<2.0.0->promptflow) (1.16.0)\n",
      "Collecting msal<2.0.0,>=1.24.0 (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Using cached msal-1.26.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions<2.0.0,>=0.3.0 (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Using cached msal_extensions-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: pycparser in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from cffi>=1.12->cryptography<42.0.0,>=41.0.3->promptflow) (2.21)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow)\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.24->promptflow)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=4.11.4->keyring<25.0.0,>=24.2.0->promptflow)\n",
      "  Using cached zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from Jinja2>=3.1.2->flask<4.0.0,>=2.2.3->promptflow)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting opencensus-context>=0.1.3 (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-api-core<3.0.0,>=1.0.0 (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Using cached google_api_core-2.17.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting importlib-metadata>=4.11.4 (from keyring<25.0.0,>=24.2.0->promptflow)\n",
      "  Using cached importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.43b0 (from opentelemetry-sdk~=1.22.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->promptflow)\n",
      "  Using cached opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai->promptflow)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.2 (from pydantic<3,>=1.9.0->openai->promptflow)\n",
      "  Downloading pydantic_core-2.16.2-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from requests->google-search-results==2.4.1->promptflow-tools) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fabon.dzogang/miniconda3/lib/python3.11/site-packages (from requests->google-search-results==2.4.1->promptflow-tools) (1.26.18)\n",
      "Collecting more-itertools (from jaraco.classes->keyring<25.0.0,>=24.2.0->promptflow)\n",
      "  Using cached more_itertools-10.2.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting google-auth<3.0.dev0,>=2.14.1 (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Downloading google_auth-2.28.1-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal<2.0.0,>=1.24.0->azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Using cached PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting portalocker<3,>=1.0 (from msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Using cached portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure<2.0.0->promptflow)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached promptflow-1.5.0-py3-none-any.whl (2.6 MB)\n",
      "Using cached promptflow_tools-1.2.0-py3-none-any.whl (41 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached flask-3.0.2-py3-none-any.whl (101 kB)\n",
      "Using cached flask_restx-1.3.0-py2.py3-none-any.whl (2.8 MB)\n",
      "Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached keyring-24.3.0-py3-none-any.whl (38 kB)\n",
      "Using cached marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "Using cached openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "Using cached opencensus_ext_azure-1.1.13-py2.py3-none-any.whl (43 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_http-1.22.0-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n",
      "Downloading pandas-2.2.0-cp311-cp311-macosx_10_9_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.2.0-cp311-cp311-macosx_10_10_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pyarrow-14.0.2-cp311-cp311-macosx_10_14_x86_64.whl (26.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pydash-7.0.7-py3-none-any.whl (110 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading SQLAlchemy-2.0.27-cp311-cp311-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached strictyaml-1.7.3-py3-none-any.whl (123 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tiktoken-0.6.0-cp311-cp311-macosx_10_9_x86_64.whl (999 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m999.8/999.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached docutils-0.20.1-py3-none-any.whl (572 kB)\n",
      "Downloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached azure_core-1.30.0-py3-none-any.whl (193 kB)\n",
      "Using cached azure_identity-1.15.0-py3-none-any.whl (164 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
      "Using cached opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n",
      "Using cached importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Using cached opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n",
      "Downloading pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.8/394.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pydantic_core-2.16.2-cp311-cp311-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading referencing-0.33.0-py3-none-any.whl (26 kB)\n",
      "Downloading regex-2023.12.25-cp311-cp311-macosx_10_9_x86_64.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rpds_py-0.18.0-cp311-cp311-macosx_10_12_x86_64.whl (335 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.8/335.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Using cached importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Using cached jaraco.classes-3.3.1-py3-none-any.whl (6.8 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached google_api_core-2.17.1-py3-none-any.whl (137 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_x86_64.whl (14 kB)\n",
      "Using cached msal-1.26.0-py2.py3-none-any.whl (99 kB)\n",
      "Using cached msal_extensions-1.1.0-py3-none-any.whl (19 kB)\n",
      "Using cached opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Downloading wrapt-1.16.0-cp311-cp311-macosx_10_9_x86_64.whl (37 kB)\n",
      "Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Using cached more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading google_auth-2.28.1-py2.py3-none-any.whl (186 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.9/186.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Building wheels for collected packages: google-search-results, greenlet\n",
      "  Building wheel for google-search-results (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-search-results: filename=google_search_results-2.4.1-py3-none-any.whl size=25774 sha256=2751ede9c0ac3c1de39c950927ece0eae1b90601556920e4b7add62b6cc21201\n",
      "  Stored in directory: /Users/fabon.dzogang/Library/Caches/pip/wheels/ad/c0/2c/36b4afda3d0d72dd07e0010b8bd868d1bbe1e65c3f3a9d4c21\n",
      "  Building wheel for greenlet (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for greenlet: filename=greenlet-3.0.3-cp311-cp311-macosx_10_9_x86_64.whl size=214635 sha256=5cd9a8f18eefa3bd1a2cdcfb3d39f69a0de3aa9b1f6738569637c06c2e582215\n",
      "  Stored in directory: /Users/fabon.dzogang/Library/Caches/pip/wheels/56/08/ca/e0bd72a4cd850a40d2a51ee67fbd2f3efe5332fe5fe00e2c53\n",
      "Successfully built google-search-results greenlet\n",
      "Installing collected packages: pytz, opencensus-context, filetype, aniso8601, zipp, wrapt, waitress, tzdata, typing-extensions, tabulate, sniffio, smmap, rpds-py, regex, python-dotenv, PyJWT, pyasn1, protobuf, portalocker, pillow, opentelemetry-semantic-conventions, numpy, more-itertools, marshmallow, MarkupSafe, itsdangerous, importlib-resources, h11, greenlet, filelock, docutils, colorama, click, cachetools, blinker, backoff, attrs, annotated-types, Werkzeug, tiktoken, strictyaml, sqlalchemy, rsa, referencing, pydash, pydantic-core, pyasn1-modules, pyarrow, pandas, opentelemetry-proto, Jinja2, jaraco.classes, importlib-metadata, httpcore, googleapis-common-protos, google-search-results, gitdb, deprecated, azure-core, anyio, pydantic, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, keyring, jsonschema-specifications, httpx, google-auth, gitpython, flask, opentelemetry-sdk, openai, msal, jsonschema, google-api-core, opentelemetry-exporter-otlp-proto-http, opencensus, msal-extensions, flask-restx, azure-identity, opencensus-ext-azure, promptflow, promptflow-tools\n",
      "Successfully installed Jinja2-3.1.3 MarkupSafe-2.1.5 PyJWT-2.8.0 Werkzeug-3.0.1 aniso8601-9.0.1 annotated-types-0.6.0 anyio-4.3.0 attrs-23.2.0 azure-core-1.30.0 azure-identity-1.15.0 backoff-2.2.1 blinker-1.7.0 cachetools-5.3.2 click-8.1.7 colorama-0.4.6 deprecated-1.2.14 docutils-0.20.1 filelock-3.13.1 filetype-1.2.0 flask-3.0.2 flask-restx-1.3.0 gitdb-4.0.11 gitpython-3.1.42 google-api-core-2.17.1 google-auth-2.28.1 google-search-results-2.4.1 googleapis-common-protos-1.62.0 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 importlib-metadata-6.11.0 importlib-resources-6.1.1 itsdangerous-2.1.2 jaraco.classes-3.3.1 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 keyring-24.3.0 marshmallow-3.20.2 more-itertools-10.2.0 msal-1.26.0 msal-extensions-1.1.0 numpy-1.26.4 openai-1.12.0 opencensus-0.11.4 opencensus-context-0.1.3 opencensus-ext-azure-1.1.13 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-http-1.22.0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 pandas-2.2.0 pillow-10.2.0 portalocker-2.8.2 promptflow-1.5.0 promptflow-tools-1.2.0 protobuf-4.25.3 pyarrow-14.0.2 pyasn1-0.5.1 pyasn1-modules-0.3.0 pydantic-2.6.1 pydantic-core-2.16.2 pydash-7.0.7 python-dotenv-1.0.1 pytz-2024.1 referencing-0.33.0 regex-2023.12.25 rpds-py-0.18.0 rsa-4.9 smmap-5.0.1 sniffio-1.3.0 sqlalchemy-2.0.27 strictyaml-1.7.3 tabulate-0.9.0 tiktoken-0.6.0 typing-extensions-4.9.0 tzdata-2024.1 waitress-2.1.2 wrapt-1.16.0 zipp-3.17.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install promptflow promptflow-tools bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promptflow\t\t\t 1.5.0\n",
      "\n",
      "Executable '/Users/fabon.dzogang/miniconda3/bin/python'\n",
      "Python (Darwin) 3.11.5 (main, Sep 11 2023, 08:19:27) [Clang 14.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "!pf -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating flow from scratch...\n",
      "Creating .promptflow folder...\n",
      "Creating README.md...\n",
      "Creating chat.jinja2...\n",
      "Creating /Users/fabon.dzogang/projects/genAI-techDevelop/coursera/promptflow/genai-techdevelop/part2/my_chatbot/flow.dag.yaml...\n",
      "Creating /Users/fabon.dzogang/projects/genAI-techDevelop/coursera/promptflow/genai-techdevelop/part2/my_chatbot/openai.yaml...\n",
      "Creating /Users/fabon.dzogang/projects/genAI-techDevelop/coursera/promptflow/genai-techdevelop/part2/my_chatbot/azure_openai.yaml...\n",
      "Creating /Users/fabon.dzogang/projects/genAI-techDevelop/coursera/promptflow/genai-techdevelop/part2/my_chatbot/requirements.txt...\n",
      "Creating /Users/fabon.dzogang/projects/genAI-techDevelop/coursera/promptflow/genai-techdevelop/part2/my_chatbot/.gitignore...\n",
      "Done. Created chat flow folder: /Users/fabon.dzogang/projects/genAI-techDevelop/coursera/promptflow/genai-techdevelop/part2/my_chatbot.\n",
      "The generated chat flow is requiring a connection named open_ai_connection, please follow the steps in README.md to create if you haven't done that.\n",
      "You can execute this command to test the flow, pf flow test --flow ./my_chatbot --interactive\n"
     ]
    }
   ],
   "source": [
    "!pf flow init --flow ./my_chatbot --type chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-21 21:59:29,317][promptflow][WARNING] - Connection with name open_ai_connection already exists. Updating it.\n",
      "{\n",
      "    \"name\": \"open_ai_connection\",\n",
      "    \"module\": \"promptflow.connections\",\n",
      "    \"created_date\": \"2024-02-15T12:09:19.259229\",\n",
      "    \"last_modified_date\": \"2024-02-21T21:59:29.318236\",\n",
      "    \"type\": \"azure_open_ai\",\n",
      "    \"api_key\": \"******\",\n",
      "    \"api_base\": \"https://uksouth.api.cognitive.microsoft.com\",\n",
      "    \"api_type\": \"azure\",\n",
      "    \"api_version\": \"2023-07-01-preview\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!pf connection create --file ./my_chatbot/azure_openai.yaml --set api_key=\"\" --set api_base=\"https://uksouth.api.cognitive.microsoft.com\" --name open_ai_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pf flow test --flow ./my_chatbot --inputs question=\"show\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-21 23:55:15 +0000   11657 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-02-21 23:55:15 +0000   11657 execution.flow     INFO     Start to run 5 nodes with concurrency level 16.\n",
      "2024-02-21 23:55:15 +0000   11657 execution.flow     INFO     Executing node extract_query_from_question. node run id: ac024847-59e6-4928-8445-83cbba40111a_extract_query_from_question_0\n",
      "2024-02-21 23:55:16 +0000   11657 execution.flow     INFO     Node extract_query_from_question completes.\n",
      "2024-02-21 23:55:16 +0000   11657 execution.flow     INFO     Executing node get_wiki_url. node run id: ac024847-59e6-4928-8445-83cbba40111a_get_wiki_url_0\n",
      "2024-02-21 23:55:17 +0000   11657 execution.flow     INFO     [get_wiki_url in line 0 (index starts from 0)] stdout> Could not find US presidential candidates 2024. Similar entity: ['2024 Democratic Party presidential candidates', '2024 Republican Party presidential primaries'].\n",
      "2024-02-21 23:55:17 +0000   11657 execution.flow     INFO     Node get_wiki_url completes.\n",
      "2024-02-21 23:55:17 +0000   11657 execution.flow     INFO     Executing node search_result_from_url. node run id: ac024847-59e6-4928-8445-83cbba40111a_search_result_from_url_0\n",
      "2024-02-21 23:55:19 +0000   11657 execution.flow     INFO     Node search_result_from_url completes.\n",
      "2024-02-21 23:55:19 +0000   11657 execution.flow     INFO     Executing node process_search_result. node run id: ac024847-59e6-4928-8445-83cbba40111a_process_search_result_0\n",
      "2024-02-21 23:55:19 +0000   11657 execution.flow     INFO     Node process_search_result completes.\n",
      "2024-02-21 23:55:19 +0000   11657 execution.flow     INFO     Executing node augmented_chat. node run id: ac024847-59e6-4928-8445-83cbba40111a_augmented_chat_0\n",
      "2024-02-21 23:55:21 +0000   11657 execution.flow     INFO     Node augmented_chat completes.\n",
      "{\n",
      "    \"answer\": \"The 2024 United States presidential election has seen a large number of candidates. For the Democratic Party, over 180 candidates filed with the Federal Election Commission. However, only a few received substantial media coverage or were elected to major public office, such as president, vice president, governor, U.S. senator, or U.S. representative. On the Republican side, the primary has been largely dominated by former President Donald Trump, with other candidates like Ron DeSantis, Nikki Haley, and Vivek Ramaswamy initially in the race but later dropping out. Trump has maintained a consistent lead in primary polling, but his potential nomination has raised some concerns within the party. Legal proceedings regarding his eligibility to appear on the ballot are ongoing in some states. For a more detailed list of candidates, I can provide sources for further information.\\n\\nSources:\\n- Democratic Party presidential candidates: https://en.wikipedia.org/w/index.php?search=2024+Democratic+Party+presidential+candidates\\n- Republican Party presidential primaries: https://en.wikipedia.org/w/index.php?search=2024+Republican+Party+presidential+primaries\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!pf flow test --flow chat_with_wikipedia --inputs question=\"Who is running for US president 2024?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pf flow test --flow functions --inputs question=\"What is the weather in Bristol?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
